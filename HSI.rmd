---
title: "Predict Hang Seng Index by ARIMA+GARCH Model"
author: "MA-Haoran"
date: "2021/03/02"
output: 
  html_document: 
    toc: yes
    theme: paper
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE)
```

## A Thank-You Letter

**Dear Admission Officer,**

**Appreciate for your time.**

**This report is a brief application of a time series analysis. I learned such mathematical theory and programming all by myself without external guide, and the document is just a personal note rather than a formal paper, so please be patient if any mistake happened. Besides, my original purpose is to apply the ARIMA+GARCH model in practice *("for fun")* rather than to build a high performance investment model, thus little financial strategy is applied here. **

**In fact, I was a full-time professional in *Ernst & Young*, one of the big4 firms, as a certified public accountant. Perhaps accounting and corporate finance is the area of my expertise. Nonetheless, I expect more from myself as I decide to expand my intellectual horizon to build an inspired mind. So here I am just willing to show my self-taught knowledge to convey my efforts, aspiration,  dedication and determination for your program.**

**It would be my honor to make every effort to realize academic value through your excellent program. Sincerely hope to see you in the near future.**


**Best regards,**

**MA Haoran**


## Introduction
The purpose is to build a time series model and predict the Hang Seng Index (HSI). 
Firstly, get the HSI index and build the training data. Then, explore the data and capture the features for a model. Find the ARIMA model with the least AIC and fit the residuals by GARCH model. Finally, make a 10 days forecast, compare the outcome with testing data.



## Process Data

```{r,message=FALSE,warning=FALSE}
#loading package
library(tidyquant) 
library(fpp3)
library(tseries)
library(rugarch)
library(data.table)
library(plotly)
setwd("C:/Users/Apple/Desktop/RStudio Tour/note/TSA/Predict-HSI-by-ARIMA-GARCH")
```



The historical data is downloaded from 
[Yahoo Finance](https://finance.yahoo.com/quote/%5EHSI/history?p=%5EHSI)(*[ctrl+click] to attach link*)
The training data covered from 2010-01-05 to 2021-01-29.
```{r}
HSI <- fread("^HSI.csv")
HSI <- HSI[, .(Date, Close = as.numeric(Close))]
HSI <- HSI %>%
  tsibble(index = Date) %>%
  mutate(Return = difference(log(HSI$Close)) * 100)

HSI <- HSI %>%
  na.omit() %>%
  mutate(time = row_number()) %>%
  update_tsibble(index = time)
head(HSI)
```

### Plot the Close Price
```{r}
HSI %>%
  ggplot(aes(x = Date, y = Close)) +
  geom_line() +
  theme_tq() +
  labs(title = "HSI Index (date data)")

```


According to the plot, the Close Price is not stationary. In order to fit an ARIMA model, it could be helpful to difference the logarithm of the initial price.

### Plot the return
Return = (ln(Close Price (t+1)) - ln(Close Price(t))) *100
```{r}
HSI %>%
  ggplot(aes(x = Date, y = Return)) +
  geom_line() +
  theme_tq() +
  labs(title = "HSI Return (date data)")


```

The Return seems stationary without seasonality.


One way to determine more objectively whether differencing is required is to use a unit root test. These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required.

### Unit Root Test

#### KPSS Test 
H0:the data is stationary around a deterministic trend.
```{r}
HSI%>%features(Return,unitroot_kpss)

```

KPSS test Pvalue = 0.1 > 0.05, so the null hypothesis is accepted.

#### ADF Test
H0: a unit root is present.

H1: time series is stationary.
```{r}
adf.test(HSI$Return, alternative = "stationary")


```
ADF test Pvalue = 0.01 < 0.05, so the null hypothesis is rejected.

In conclusion, the time series is stationary for an ARIMA model.



## Model:ARIMA+GARCH
### ARIMA part

#### Find the ARIMA model with the least AIC.

```{r}
arima.mod <- HSI %>%
    model(ARIMA(Return, stepwise = FALSE, approximation = FALSE))

arima.mod
```


#### ARIMA residual analysis:
```{r}
arima.mod%>%
    gg_tsresiduals()


```


#### Ljung-box Test

H0: Data cannot be distinguished from white noise.

```{r}
augment(arima.mod)%>%features(.innov,ljung_box)
```

According to the plot and ljung-box test, pvalue>0.05, so the residuals of the model cannot be distinguished from white noise. 
It is rational to accept **ARIMA(2,0,3)**.



#### Detect the ARCH effect on residuals:
There would be an ARCH effect if the acf and pacf of residuals^2 are significant.
```{r}
p1 <- arima.mod %>%
  augment() %>%
  select(.innov) %>%
  ACF(.innov ^ 2) %>%
  autoplot() +
  labs(y = "resid^2  acf")

p2 <- arima.mod %>%
  augment() %>%
  select(.innov) %>%
  PACF(.innov ^ 2) %>%
  autoplot() +
  labs(y = "resid^2  pacf")

gridExtra::grid.arrange(p1, p2)


```


Thus the ARCH effect on residuals is significant, it plausible to fit a GARCH model on residuals.


### GARCH part
Try model: ARIMA(2,0,3)+GARCH(1,1)
```{r}
spec <-
  ugarchspec(
    variance.model = list(
      model = "sGARCH",
      garchOrder = c(1, 1),
      submodel = NULL,
      external.regressors = NULL,
      variance.targeting = FALSE
    ),
    mean.model = list(armaOrder = c(2, 3),
                      include.mean = TRUE),
    distribution.model = "sged"
  )


fit <- ugarchfit(spec, HSI$Return,
                 solver = "hybrid")
```

Report the final model 
```{r}
print(fit)
```

## Forecast
Build a forecast function to predict return in 10 days.
```{r}
fore <- function(input, p, q, n) {
  spec <-
    ugarchspec(
      variance.model = list(
        model = "sGARCH",
        garchOrder = c(1, 1),
        submodel = NULL,
        external.regressors = NULL,
        variance.targeting = FALSE
      ),
      mean.model = list(armaOrder = c(p, q),
                        include.mean = TRUE),
      distribution.model = "sged"
    )
  fit <- ugarchfit(spec, input$Return,
                   solver = "hybrid")
  fore <- ugarchforecast(fit, n.ahead = n)
  pred <- fore@forecast$seriesFor
  pred <- as.data.table(pred)
  setnames(pred, "Return")
  pred[, time := seq.int(from = 1, to = n, by = 1)][]
  
}


pred <- fore(HSI, 2, 3, 10)

#View the predictions
pred




```

Examine the predictions with testing set. The raw test data starts from 2021-01-29, the last day of the traing set.
```{r}
test <- fread("test.csv")
test <- test[, .(Date, Close = as.numeric(Close))]
test <- test %>%
  tsibble(index = Date) %>%
  mutate(Return = difference(log(test$Close)) * 100)

test <- test %>%
  na.omit() %>%
  mutate(time = row_number()) %>%
  update_tsibble(index = time)

# Preview the test data
head(test, 5)


```

Plot forecast value and test value.
```{r}

foretest <- function(pred, test) {
  p <- pred %>%
    tsibble(index = time) %>%
    left_join(test, by = "time") %>%
    mutate(Pred = Return.x,
           Actual = Return.y) %>%
    select(time, Date, Close, Pred, Actual) %>%
    pivot_longer(
      cols = c(Pred, Actual),
      values_to = "Return",
      names_to = "Type"
    ) %>%
    ggplot(aes(x = Date, y = Return, color = Type)) +
    geom_line() +
    scale_x_date(date_minor_breaks = "1 day") +
    ggsci::scale_color_aaas()+
    labs(title = "10 Trading Days Forecast")
  ggplotly(p)
}

foretest(pred, test)

```

Discrepancy could easily be observed, the actual curve seems 1 lag behind the prediction curve. However, the model is still capable of capturing some trend since the locations of inflection points are pretty close. Also, as the forecast period grows, the prediction curve gradually smooths.

(Perhaps the deviation is caused by efficient market, which maybe a good news for society. Just kidding!)

Of course the model has to be improved. Cross-validation would be useful. It might be fun to use bootstrap and function hilo() to draw the prediction interval.





## Cross Validation

From year 2020 to 2021, splice every 100 trading days and train data and make a one day forecast.

For instance,

train Day1~Day100, forecast Day101;
train Day2~Day101, forecast Day102;
train Day3~Day103, forecast Day103,etc.


#### Get the ARMA p,q with the least AIC for "year > 2020"
```{r,cache=TRUE}
CrossValidation <- HSI %>%
  filter(year(Date) >= 2020)

buildARIMA <- function(input) {
    arima.mod <- input %>%
        model(ARIMA(Return, stepwise = FALSE, approximation = FALSE))
    
    list(
        model = arima.mod,
        ljung_box = augment(arima.mod) %>% features(.innov, ljung_box)
    )
}

buildARIMA(CrossValidation)



```


According to the result, ARMA(3,1) is acceptable.

#### Cross Validation Test
ARIMA(3,0,1)+GARCH(1,1)
```{r}
CrossValidation <- CrossValidation %>%
  tsibble(index = time) %>%
  stretch_tsibble(.step = 1, .init = 100, .id = ".id") %>%
  group_by_key() %>%
  slice(n() - 99:0)
 spec <-
            ugarchspec(
                variance.model = list(
                    model = "sGARCH",
                    garchOrder = c(1, 1),
                    submodel = NULL,
                    external.regressors = NULL,
                    variance.targeting = FALSE
                ),
                mean.model = list(armaOrder = c(3, 1),
                                  include.mean = TRUE),
                distribution.model = "sged"
            )

 
 
 
 Crossforecast <- function(data) {
   CrossFore <- vector()
   for (i in 1:tail(data$.id)[1]) {
     train <- data %>%
       filter(.id == i)
     
     fit <- ugarchfit(spec, train$Return,
                      solver = "hybrid")
     
     CrossFore[i] <-
       ugarchforecast(fit, n.ahead = 1)@forecast$seriesFor[1]
     
   }
   CrossFore
 }

 CrossPred<-Crossforecast(CrossValidation)
 
 tsCV<-HSI %>%
    filter(year(Date) >= 2020)%>%
    mutate(time=row_number())%>%
    filter(time>100)%>%
   mutate(Pred=CrossPred[1:(length(CrossPred)-1)])
 head(tsCV)
```

#### Plot the result
```{r}
tsCV %>%
  pivot_longer(
    cols = c(Pred, Return),
    values_to = "Return",
    names_to = "Type"
  ) %>%
  ggplot(aes(x = Date, y = Return, color = Type)) +
  geom_line() +
  scale_x_date(date_minor_breaks = "1 day") +
  theme_tq() +
  labs(title = "Cross Validation Test")
  




```


Weakness:

The fluctuation of the forecast curve is significantly weaker than the actual curve.


Advantage:

The trend of the prediction curve is informative. Also, it is harmless to determine whether return is positive or negative by forecast value after June 2020.




*To assure the reproducibility, all code and data have been uploaded to [Github](https://github.com/ma-haoran/Predict-HSI-by-ARIMA-GARCH)*(*[ctrl+click] to attach link*)

